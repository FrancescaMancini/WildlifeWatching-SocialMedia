---
title: "AnalysisReport"
author: "Francesca Mancini"
date: "28 February 2017"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align="center")
```

Load required libraries and dataset.

```{r load_data}
library(gstat)
library(MASS)
library(sp)

data_sub<-read.table(".//data//CombinedData_v11.txt",stringsAsFactors = F,header=T)

```

Check for collinearity between linear predictors. 
First create dataframes containing only the predictors of interest. Our approach is to fit a model with higher level variables (Area_PA, Count_Inf and Mean_Nat), then separate models for infrastructure and environmental variables. So we create 3 dataframes of predictors.
Then calculate variance inflation factor (VIF).

```{r Agg_VIF}
preds.agg<-data_sub[,c("Area_PA","Mean_Nat","Count_Inf")]

#then calculate variance inflation factor

#calculate VIF
source("Collinearity.R")
VIF<-corvif(preds.agg)

```

The re is no collinearity between the aggregated variables.

```{r Env_VIF}
preds_env <- data_sub[,c("Area_WHS", "Area_SSSI", "Area_SPA", "Area_SAC_L", "Area_RAMSA", "Area_NR", "Area_NNR",
                   "Dist_MPA", "Dist_MCA", "Area_LNR", "Area_COUNE", "Area_CNTRY", "Area_BIOSP", "Area_BIOGE",
                   "Area_NP", "Mean_Nat", "Dist_MSAC")]
#calculate VIF
source("Collinearity.R")
VIF_env <- corvif(preds_env)

```

The variables Area_SSSI and Area_SAC_L are highly collinear (correlation = 0.8 and VIF > 3). Therefore they cannot be included in the same model.

```{r Env_VIF2}
# Area_SSSI and Area_SAC_L are highly collinear (VIF > 3)

preds_env <- preds_env[,-4]

VIF_env <- corvif(preds_env)
# all VIF < 2

```


We fit the full model with only Area_SAC_L. Later we will do the model selection taking into account this collinearity issue.
All predictors are scaled to avoid numerical issues due to values being very far from 0. We also log the response variable because the distribution of Count_WW is highly skewed with a long tail. This distribution is very problematic and both a Poisson and Negative Binomial models were tested and resulted in a poor fit. 

```{r hist}
hist(data_sub$Count_WW,breaks=10000)
```

First we transform the coordinates from lat and long to utm so the distance unit on the y axis is the same as on the x axis. This avoids problems with estimating the correct correlation structure.

```{r coordsTransform}
#transform coordinates from latlong to utm to avoid issues with correlation structures

library(rgdal)

coordinates(data_sub) <- data_sub[,c(29,30)]
crs.geo <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")  # geographical, datum WGS84

proj4string(data_sub) <- crs.geo                             # assign the coordinate system

coords_proj <- spTransform(data_sub, CRS("+proj=utm +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 "))

data_sub<-cbind(data_sub@data, coords_proj@coords)

names(data_sub)[c(51,52)] <- c("x","y")

```

```{r full.lm}
full.lm<-lm(log10(Count_WW+1)~scale(Area_WHS,scale=T) + scale(Area_SPA,scale=T) + 
              scale(Area_SSSI,scale=T) +scale(Area_RAMSA,scale=T) + scale(Area_NR,scale=T) +
              scale(Area_NNR,scale=T) + scale(Dist_MPA,scale=T) +scale(Dist_MSAC,scale=T) +
              scale(Dist_MCA,scale=T) + scale(Area_LNR,scale=T) + scale(Area_COUNE,scale=T) +
              scale(Area_CNTRY,scale=T) + scale(Area_BIOSP,scale=T) +
              scale(Area_BIOGE,scale=T) + scale(Area_NP,scale=T) + scale(Dist_Air,scale=T) + 
              scale(Count_Bus,scale=T) + scale(Count_Hotel,scale=T) + scale(Dist_CarPark,scale=T) + 
              scale(Dist_TourOp,scale=T) +  scale(Dist_Train,scale=T)  + scale(Dist_road,scale=T) + 
              scale(Mean_Nat,scale=T),data=data_sub)


#look at the summary
summary(full.lm)


```

We check that model assumptions are met by extracting the model residuals and plotting them against the fitted values and by looking at their qqplot.

```{r full.lm_validation1, fig.height=6, fig.width=8}
S.res.lm<-rstandard(full.lm)
fit.lm<-fitted(full.lm)

par(mfrow=c(1,2))
plot(S.res.lm~fit.lm)

```

Normality assumption is met, but there is a pattern in the residuals vs fitted value plot: positive residuals for low fitted values and negative residuals for high fitted values.
The data might be spatially correlated so we check the autocorrelation in the residuals with a variogram 

```{r full.lm_validation2, fig.height=6, fig.width=8}
#create a spatial dataframe
mydata_sp<-data_sub
coordinates(mydata_sp)<-c("x", "y")

#calculate and plot variograms
Vario<-variogram(S.res.lm~1,data=mydata_sp)
Variodir<-variogram(S.res.lm~1,data=mydata_sp,alpha=c(0,45,90,135))

plot(Vario)
plot(Variodir)

```

and a bubbleplot.

```{r full.lm_bubbleplot, fig.height=6, fig.width=8}
#make a bubble plot of the residuals to check for spatial patterns
bubble.data<-data.frame(S.res.lm,data_sub$x,data_sub$y)
coordinates(bubble.data)<-c("data_sub.x","data_sub.y")

bubble(bubble.data,"S.res.lm",col=c("black","grey"),main="Residuals",xlab="Longitude",ylab="Latitude")

```

Both the variogram and the bubbleplot show that the residuals are spatially correlated. The directional variograms show that isotropy is a reasonable assumption so we can use a simple correlation structure in a gls framework to account for spatial dependency in the residuals.

### GLS

Load the required library.

```{r nlme_lib}
library(nlme)
```

First we create a new dataframe with all the transformed variables.

```{r gls.data}
#create a new dataframe with scaled predictors and logged response
gls.data<-data.frame(Count_WW=log10(data_sub$Count_WW+1),Area_WHS=scale(data_sub$Area_WHS,scale=T),
                     Area_SPA=scale(data_sub$Area_SPA,scale=T), Dist_MSAC=scale(data_sub$Dist_MSAC,scale=T),
                    Area_SSSI=scale(data_sub$Area_SSSI,scale=T), Area_RAMSA=scale(data_sub$Area_RAMSA,scale=T),
                     Area_NR=scale(data_sub$Area_NR,scale=T), Area_NNR=scale(data_sub$Area_NNR,scale=T), 
                     Dist_MPA=scale(data_sub$Dist_MPA,scale=T), Dist_MCA=scale(data_sub$Dist_MCA,scale=T), 
                     Area_LNR=scale(data_sub$Area_LNR,scale=T), Area_COUNE=scale(data_sub$Area_COUNE,scale=T),
                     Area_CNTRY=scale(data_sub$Area_CNTRY,scale=T), Area_BIOSP=scale(data_sub$Area_BIOSP,scale=T), 
                     Area_BIOGE=scale(data_sub$Area_BIOGE,scale=T), Area_NP=scale(data_sub$Area_NP,scale=T), 
                     Dist_Air=scale(data_sub$Dist_Air,scale=T), Count_Bus=scale(data_sub$Count_Bus,scale=T),
                     Count_Hotel=log10(data_sub$Count_Hotel+1), Dist_CarPark=scale(data_sub$Dist_CarPark,scale=T), 
                     Dist_TourOp=scale(data_sub$Dist_TourOp,scale=T), Dist_Train=scale(data_sub$Dist_Train,scale=T), 
                     Dist_road=scale(data_sub$Dist_road,scale=T), Mean_Nat=scale(data_sub$Mean_Nat,scale=T),
                     Area_PA = scale(data_sub$Area_PA, scale = T), Count_Inf = scale(data_sub$Count_Inf,scale = T),
                     x=data_sub$x,y=data_sub$y, Pop_dens = data_sub$Pop_dens, 
                     Area_SAC_L = scale(data_sub$Area_SAC_L, scale = T))
```


### High-level model

First we can fit a model with aggregated variables: the total area of the cell that is occupied by a reserve, the total number of infrastructures and the mean naturalness.
This will test whether environmental, socio-economic infrastrcture or the naturalness is the strogest attractor for tourists.
We first find the best correlation structure.

```{r agg.gls, cache=TRUE}
agg.gls.ratio<-gls(Count_WW ~ Area_PA + Mean_Nat + Count_Inf,data=gls.data,
             correlation=corRatio(form=~x+y,nugget=T))

agg.gls.sph<-gls(Count_WW ~ Area_PA + Mean_Nat + Count_Inf,data=gls.data,
                   correlation=corSpher(form=~x+y,nugget=T))

agg.gls.Lin<-gls(Count_WW ~ Area_PA + Mean_Nat + Count_Inf,data=gls.data,
                 correlation=corLin(form=~x+y,nugget=T))

agg.gls.Gaus<-gls(Count_WW ~ Area_PA + Mean_Nat + Count_Inf,data=gls.data,
                 correlation=corGaus(form=~x+y,nugget=T))

agg.gls.exp<-gls(Count_WW ~ Area_PA + Mean_Nat + Count_Inf,data=gls.data,
                  correlation=corExp(form=~x+y,nugget=T))


AIC(agg.gls.ratio, agg.gls.sph, agg.gls.Lin, agg.gls.Gaus, agg.gls.exp)

```

Check the validity of the model. First we look for patterns in the residuals.

```{r agg.gls.valid}
#check for patterns in the residuals

res.agg.gls<-residuals(agg.gls.exp,type="normalized")
fit.agg.gls<-fitted(agg.gls.exp)

plot(res.agg.gls~fit.agg.gls)

qqnorm(res.agg.gls)
qqline(res.agg.gls)

```

Then we check that autocorrelation is not an issue anymore.

```{r agg.gls.valid2}
#check that autocorrelation is not an issue anymore
Vario.agg.gls<-Variogram(agg.gls.exp,form= ~x+y,robust=T,resType = "normalized")

plot(Vario.agg.gls, smooth=F)

```

Then look at the summary.

```{r agg.gls.summary}
summary(agg.gls.exp)
```

All the variables seem to be imprtant in explaining the distribution of the wildlife watchers.
We can now fit an environmental model and an infrastructure model to select which variables are important in the two groups.

##### Environmental model

Again we first select the best correlation structure.

```{r env.gls, cache=TRUE}
env.gls <- gls(Count_WW~ Area_WHS + Area_SAC_L + Area_SSSI + Dist_MSAC +Dist_MPA +Dist_MCA +
                 Area_SPA +Area_RAMSA+ Area_NR + Area_NNR+   Area_LNR + Area_COUNE + 
                 Area_CNTRY + Area_BIOSP + Area_BIOGE + Area_NP + Mean_Nat,data=gls.data)

env.gls.exp<-gls(Count_WW~ Area_WHS + Area_SAC_L + Area_SSSI + Dist_MSAC +Dist_MPA +Dist_MCA +
               Area_SPA +Area_RAMSA+ Area_NR + Area_NNR+   Area_LNR + Area_COUNE + 
               Area_CNTRY + Area_BIOSP + Area_BIOGE + Area_NP + Mean_Nat,
               data=gls.data, correlation=corExp(form=~x+y,nugget=T))

env.gls.ratio<-gls(Count_WW~ Area_WHS + Area_SAC_L + Area_SSSI + Dist_MSAC +Dist_MPA +Dist_MCA +
               Area_SPA +Area_RAMSA+ Area_NR + Area_NNR+   Area_LNR + Area_COUNE + 
               Area_CNTRY + Area_BIOSP + Area_BIOGE + Area_NP + Mean_Nat,
               data=gls.data, correlation=corRatio(form=~x+y,nugget=T))

env.gls.sph<-gls(Count_WW~ Area_WHS + Area_SAC_L + Area_SSSI + Dist_MSAC +Dist_MPA +Dist_MCA +
                 Area_SPA +Area_RAMSA+ Area_NR + Area_NNR+   Area_LNR + Area_COUNE + 
                 Area_CNTRY + Area_BIOSP + Area_BIOGE + Area_NP + Mean_Nat,
                 data=gls.data, correlation=corSpher(form=~x+y,nugget=T))

env.gls.Lin<-gls(Count_WW~ Area_WHS + Area_SAC_L + Area_SSSI + Dist_MSAC +Dist_MPA +Dist_MCA +
                 Area_SPA +Area_RAMSA+ Area_NR + Area_NNR+   Area_LNR + Area_COUNE + 
                 Area_CNTRY + Area_BIOSP + Area_BIOGE + Area_NP + Mean_Nat,
                 data=gls.data,correlation=corLin(form=~x+y,nugget=T))

env.gls.Gaus<-gls(Count_WW~ Area_WHS + Area_SAC_L + Area_SSSI + Dist_MSAC +Dist_MPA +Dist_MCA +
                  Area_SPA +Area_RAMSA+ Area_NR + Area_NNR+   Area_LNR + Area_COUNE + 
                  Area_CNTRY + Area_BIOSP + Area_BIOGE + Area_NP + Mean_Nat,
                  data=gls.data,correlation=corGaus(form=~x+y,nugget=T))

AIC(env.gls,env.gls.exp, env.gls.ratio, env.gls.sph, env.gls.Lin, env.gls.Gaus)

```

Check model assumptions.

```{r env.gls.valid}
#check for patterns in the residuals

res.env.gls<-residuals(env.gls.exp,type="normalized")
fit.env.gls<-fitted(env.gls.exp)

plot(res.env.gls~fit.env.gls)

qqnorm(res.env.gls)
qqline(res.env.gls)

```

Check for autocorrelation.

```{r env.gls.valid2}
#check that autocorrelation is not an issue anymore
Vario.env.gls<-Variogram(env.gls.exp,form= ~x+y,robust=T,resType = "normalized")

plot(Vario.env.gls, smooth=F)

summary(env.gls.exp)

```

And summary.

```{r env.gls.summary}
summary(env.gls.exp)
```


##### Infrastructure model

Selecting the best correlation structure.

```{r inf.gls, cache=TRUE}
inf.gls <- gls(Count_WW ~ Dist_Air + Count_Bus + Count_Hotel + Dist_CarPark + Dist_TourOp +
                 Dist_Train  + Dist_road,data=gls.data)

inf.gls.exp<-gls(Count_WW ~ Dist_Air + Count_Bus + Count_Hotel + Dist_CarPark + Dist_TourOp +
                 Dist_Train  + Dist_road,data=gls.data, correlation=corExp(form=~x+y,nugget=T))

inf.gls.sph<-gls(Count_WW ~ Dist_Air + Count_Bus + Count_Hotel + Dist_CarPark + Dist_TourOp +
                 Dist_Train  + Dist_road,data=gls.data, correlation=corSpher(form=~x+y,nugget=T))

inf.gls.ratio<-gls(Count_WW ~ Dist_Air + Count_Bus + Count_Hotel + Dist_CarPark + Dist_TourOp +
                   Dist_Train  + Dist_road,data=gls.data, correlation=corRatio(form=~x+y,nugget=T))

inf.gls.Lin<-gls(Count_WW ~ Dist_Air + Count_Bus + Count_Hotel + Dist_CarPark + Dist_TourOp +
                 Dist_Train  + Dist_road,data=gls.data, correlation=corLin(form=~x+y,nugget=T))

inf.gls.Gaus<-gls(Count_WW ~ Dist_Air + Count_Bus + Count_Hotel + Dist_CarPark + Dist_TourOp +
                  Dist_Train  + Dist_road,data=gls.data, correlation=corGaus(form=~x+y,nugget=T))

AIC(inf.gls,inf.gls.exp, inf.gls.sph, inf.gls.ratio, inf.gls.Lin, inf.gls.Gaus)

```

Model validation.

```{r inf.gls.valid}
#check for patterns in the residuals

res.inf.gls<-residuals(inf.gls.exp,type="normalized")
fit.inf.gls<-fitted(inf.gls.exp)

plot(res.inf.gls~fit.inf.gls)

qqnorm(res.inf.gls)
qqline(res.inf.gls)

```

Check for residual autocorrelation.

```{r inf.gls.valid2}
#check that autocorrelation is not an issue anymore
Vario.inf.gls<-Variogram(inf.gls.exp,form= ~x+y,robust=T,resType = "normalized")

plot(Vario.inf.gls, smooth=F)

```

And summary.

```{r inf.gls.summary}
summary(inf.gls.exp)
```

###Model selection using dredge

We use the pdredge function in package MuMIn to select iportant variables in the models.

##### Infrastructures model

The function pdredge allows parallel computing.
The following code sets up a cluster and runs the dredge function on 4 cores.

```{r pdredgeInf, eval=FALSE}
#use pdredge to use parallell computing
library(parallel)
library(MuMIn)

# Calculate the number of cores
cores <- detectCores()

# Set up a cluster with number of cores specified as result of detectCores() 
#   and call it "clust" 
clust <- makeCluster(cores)


# Load required packages onto worker nodes
#   (in this example, load packages {nlme} and {MuMIn} to be used by pdredg)
clusterEvalQ(clust,library(nlme))
clusterEvalQ(clust,library(MuMIn))
clusterExport(clust,"gls.data")  #export the dataframe to the cluster
clusterExport(clust,"inf.gls.exp")  #export the model to the cluster

#model selection for infrastructure model
inf.sel<-pdredge(inf.gls.exp, cluster=clust,rank = "AICc",trace=2, REML = FALSE)

saveRDS(inf.sel,"Infrastructure_sel.rds")

stopCluster(clust)

```

```{r readModSel, include=FALSE}
library(MuMIn)
inf.sel<-readRDS("Infrastructure_sel.rds")
```

Take a subset of the models that are within 5 delta AIC from the best model and refit them using REML.

```{r InfREML, eval = FALSE}
#refit subset of models with REML
inf.sel.REML<-get.models(inf.sel, subset = delta < 5)

```

```{r readBestInfMod, include=FALSE}
inf.sel.REML<-readRDS("Infrastructure_Best.rds")
```

Now we cal look at the importance of different variables based on the weights from the model selection.

```{r Inf_VarImp, message=FALSE, warning=FALSE}
inf.var.imp<-importance(inf.sel.REML)

# put it into a dataframe format

df <- as.data.frame(inf.var.imp)

inf.var.imp <- cbind(df, attr(inf.var.imp, "names"))

names(inf.var.imp) <- c("Importance", "Var")

# and plot

library(ggplot2)

InfVarImpVis <- ggplot (inf.var.imp, aes(Var, Importance, fill = Importance)) +
  geom_hline(yintercept = seq(0, 1.2, by = 0.5), colour = "grey90", size = 1) +
  geom_vline(aes(xintercept = Var), colour = "grey90", size = 1) +
  geom_bar(width = 1, stat = "identity", color = "white") +
  scale_y_continuous(breaks = 0:nlevels(as.factor(inf.var.imp$Var))) +
  scale_fill_gradient(low = "thistle1", high = "thistle4") +
  theme_bw() +
  theme(axis.title = element_blank(),
        panel.border = element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_blank(),
        panel.grid = element_blank(),
        axis.text.x = element_text(size = 12),
        legend.title = element_text(size = 15),
        legend.text = element_text(size = 12))

InfVarImpVis +coord_polar()

```

Now we can use model averaging to get coefficient estimates averaged across the subset of models and calculate confidence intervals.

```{r ModAvg}
#model averaging
inf.avg<-model.avg(inf.sel.REML, revised.var = TRUE) 
summary(inf.avg) 

inf.confint<-confint(inf.avg)

```

##### Environmental model

The following code was used to set up a cluster and run the model selection on the environmental variables on the UoA HPC Maxwell. For bash script see Env_ModSel_bash.txt in this repository.

```{r pdredgeEnv, eval = FALSE}
library(nlme)
library(MuMIn)
library(snow)
library(snowfall)
library(Rmpi)

gls.data<-read.table("gls.data.txt",stringsAsFactors = F,header=T)

env.gls.exp<-gls(Count_WW~ Area_WHS + Area_SAC_L + Area_SSSI + Dist_MSAC +Dist_MPA +Dist_MCA +
               Area_SPA +Area_RAMSA+ Area_NR + Area_NNR+   Area_LNR + Area_COUNE + 
               Area_CNTRY + Area_BIOSP + Area_BIOGE + Area_NP + Mean_Nat,
               data=gls.data, correlation=corExp(form=~x+y,nugget=T))
            
clust <- makeCluster(mpi.universe.size())

# Load required packages onto worker nodes
#   (in this example, load packages {nlme} and {MuMIn} to be used by pdredge)
clusterEvalQ(clust,library(nlme))
clusterEvalQ(clust,library(MuMIn))
clusterExport(clust,"gls.data")  #export the dataframe to the cluster
clusterExport(clust,"env.gls.exp")  #export the model to the cluster

env.sel<-pdredge(env.gls.exp, cluster=clust, rank = "AICc",trace=2, REML = FALSE,
                 subset = !("Area_SSSI"&& "Area_SAC_L")) #do not include collinear variables in the same model

saveRDS(env.sel,"Environment_sel.rds")

stopCluster(clust)
```

The same procedure used for the infrastructure model was followed to perform model averaging for the environmental model. For R code see script Analysis.R.

### Biodiversity 

The spatial distribution of the biodiversity records is overlapping that of Flickr data. There are areas that lack access infrastructure where people are not able to go to record biodiversity or for recreational purposes. So we exclude these areas from this analysis. Once we account for infrastructure and amenities, is species richness an attractor in areas that are easily accessible? 

We first look at the distribution of the biodiversity records.

```{r BioDistr}
par(mfrow=c(2,1))
hist(log(data_sub$Records+1),main="Histogram of species records",xlab="Species Records (log)")
plot(density(log(data_sub$Records+1)),main="Density of species records")
```

There is obviously a bimodal distribution with most of the cells having 0 records and then the distribution has another peak around 54.

Now we look at the distribution of species richness.

```{r SpecRichDist}
par(mfrow=c(2,1))
hist(log(data_sub$Species+1),main="Histogram of species richness",xlab="Species Richness (log)")
plot(density(log(data_sub$Species+1)),main="Density of species richness")
```

Very similar story for the species richness.


#### Subsetting
We can identify the two distributions that these data come from by using a mixture model.

```{r MixMod, eval=FALSE}
library(mclust)

#create variable log of number of records
data_sub$logrec<-log(data_sub$Records+1)

#Produces a density estimate for each data point using a Gaussian finite mixture model from Mclust.
clusrec<-densityMclust(data_sub$logrec,G=2)     #G=2 we know that there are 2 groups (low and high)

#plot the distribution
plot(clusrec, what = "density", data = data_sub$logrec, breaks = 15)

#Very high nuber of 0s 

#remove 0 records
sublogrec<-data_sub$logrec[data_sub$logrec>0]

#refit mixture model without 0s
clusrecsub<-densityMclust(sublogrec,G=2)

#plot
plot(clusrecsub, what = "density", data = sublogrec, breaks = 15)

#returns a summary of the model
classification<- summary(clusrecsub,classification=T)

#classification of each datapoint into the two groups is then in:
membership<-classification$classification

table(membership)
write.table(membership,"data/classification.txt")
```


Read classification from mixture model

```{r readClass}
classes<-read.table("data/classification.txt")
```


Now we use this classification to subset our dataset and only select records that are in an area that is accessible to people.

```{r Subset}
data_sub$logrec<-log(data_sub$Records+1)

#only select non 0 records
data.pos<-data_sub[data_sub$logrec>0,]

#now select only records belonging to group 2
names(classes)<-"clas"
data.gr2<-cbind(data.pos,classes)
data.gr2<-data.gr2[data.gr2$clas==2,]
#str(data.gr2)
```

#### Effect of biodiversity

Now we can model the effect of biodiversity. First we try a simple linear model with both variables logtransformed.

```{r BioGls}
gls.data.bio<-data.frame(Count_WW=log10(data.gr2$Count_WW+1), Dist_MSAC=scale(data.gr2$Dist_MSAC,scale=T),
                     Area_LNR=scale(data.gr2$Area_LNR,scale=T), Area_CNTRY=scale(data.gr2$Area_CNTRY,scale=T), 
                     Area_NP=scale(data.gr2$Area_NP,scale=T), Mean_Nat=scale(data.gr2$Mean_Nat,scale=T),
                     Species=log10(data.gr2$Species), x=data.gr2$x,y=data.gr2$y)

bio.gls.exp<-gls(Count_WW ~ Dist_MSAC + Area_LNR + Area_CNTRY + Area_NP + Mean_Nat + Species,
                 data = gls.data.bio, correlation = corExp(form = ~ x+y,nugget = T))

bio.gls.lin<-gls(Count_WW ~ Dist_MSAC + Area_LNR + Area_CNTRY + Area_NP + Mean_Nat + Species,
                 data = gls.data.bio, correlation = corLin(form = ~ x+y,nugget = T))

bio.gls.ratio<-gls(Count_WW ~ Dist_MSAC + Area_LNR + Area_CNTRY + Area_NP + Mean_Nat + Species,
                   data = gls.data.bio, correlation = corRatio(form = ~ x+y,nugget = T))

bio.gls.gaus<-gls(Count_WW ~ Dist_MSAC + Area_LNR + Area_CNTRY + Area_NP + Mean_Nat + Species,
                  data = gls.data.bio, correlation = corGaus(form = ~ x+y,nugget = T))

bio.gls.sph<-gls(Count_WW ~ Dist_MSAC + Area_LNR + Area_CNTRY + Area_NP + Mean_Nat + Species,
                 data = gls.data.bio, correlation = corSpher(form = ~ x+y,nugget = T))

```

Now we use AIC to determine the best autocorrelation structure.

```{r BioModSel}
AIC(bio.gls.exp, bio.gls.lin, bio.gls.ratio, bio.gls.gaus, bio.gls.sph)
```

The best model includes the exponantial correlation structure.

```{r BioBest}
summary(bio.gls.exp)
```


Check model assumptions.

```{r BioLmCheck}
res.bio.gls<-residuals(bio.gls.exp, type="normalized")

fit.bio.gls<-fitted(bio.gls.exp)

par(mfrow=c(1,2))
plot(res.bio.gls ~ fit.bio.gls)

qqnorm(res.bio.gls)

qqline(res.bio.gls)

```

Check for spatial autocorrelation by plotting a variogram.


```{r BioLmSpatialCheck}
mydata_sp<-gls.data.bio
coordinates(mydata_sp)<-c("x", "y")

Vario<-variogram(res.bio.gls~1,data=mydata_sp)

plot(Vario)
```

