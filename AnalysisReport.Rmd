---
title: "AnalysisReport"
author: "Francesca Mancini"
date: "28 February 2017"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align="center")
```

Load required libraries and dataset.

```{r load_data}
library(gstat)
library(MASS)
library(sp)

data_sub<-read.table(".//data//CombinedData_v11.txt",stringsAsFactors = F,header=T)

```

Check for collinearity between linear predictors. 
First create a dataframe containing only the predictors of interest.
Then calculate variance inflation factor (VIF).

```{r VIF}
preds<-data_sub[,c("Area_WHS","Area_SSSI","Area_SPA","Area_SAC_L","Area_RAMSA","Area_NR","Area_NNR",
                   "Dist_MPA","Dist_MCA","Area_LNR","Area_COUNE","Area_CNTRY","Area_BIOSP","Area_BIOGE",
                   "Area_NP","Dist_Air","Count_Bus","Count_Hotel","Dist_CarPark","Dist_TourOp",
                   "Dist_Train","Dist_road","Mean_Nat","Dist_MSAC")]

source("Collinearity.R")
VIF<-corvif(preds)

```

The variables Area_SSSI and Area_SAC_L are highly collinear (correlation = 0.8 and VIF > 3). Therefore they cannot be included in the same model.
We fit the full model with only Area_SAC_L. Later we will fit 2 different models to include the variable Area_SSSI as well.
All predictors are scaled to avoid numerical issues due to values being vey far from 0. We also log the response variable because the distribution of Count_WW is highly skewed with a long tail. This distribution is very problematic and both a Poisson and Negative Binomial models were tested and resulted in a poor fit. 

```{r hist}
hist(data_sub$Count_WW,breaks=10000)
```


```{r full.lm}
full.lm<-lm(log10(Count_WW+1)~scale(Area_WHS,scale=T) + scale(Area_SPA,scale=T) + 
              scale(Area_SAC_L,scale=T) +scale(Area_RAMSA,scale=T) + scale(Area_NR,scale=T) +
              scale(Area_NNR,scale=T) + scale(Dist_MPA,scale=T) +scale(Dist_MSAC,scale=T) +
              scale(Dist_MCA,scale=T) + scale(Area_LNR,scale=T) + scale(Area_COUNE,scale=T) +
              scale(Area_CNTRY,scale=T) + scale(Area_BIOSP,scale=T) +
              scale(Area_BIOGE,scale=T) + scale(Area_NP,scale=T) + scale(Dist_Air,scale=T) + 
              scale(Count_Bus,scale=T) + scale(Count_Hotel,scale=T) + scale(Dist_CarPark,scale=T) + 
              scale(Dist_TourOp,scale=T) +  scale(Dist_Train,scale=T)  + scale(Dist_road,scale=T) + 
              scale(Mean_Nat,scale=T) + offset(log10(Pop_dens+1)),data=data_sub)

summary(full.lm)

```

We check that model assumptions are met by extracting the model residuals and plotting them against the fitted values and by looking at their qqplot.

```{r full.lm_validation1, fig.height=6, fig.width=8}
S.res.lm<-rstandard(full.lm)
fit.lm<-fitted(full.lm)

par(mfrow=c(1,2))
plot(S.res.lm~fit.lm)

qqnorm(S.res.lm)
qqline(S.res.lm)

```

Normality assumption is met, but there is a pattern in the residuals vs fitted value plot: positive residuals for low fitted values and negative residuals for high fitted values.
The data might be spatially correlated so we check the autocorrelation in the residuals with a variogram 

```{r full.lm_validation2, fig.height=6, fig.width=8}
mydata_sp<-data_sub
coordinates(mydata_sp)<-c("Longitude", "Latitude")

#calculate and plot variograms
Vario<-variogram(S.res.lm~1,data=mydata_sp)
Variodir<-variogram(S.res.lm~1,data=mydata_sp,alpha=c(0,45,90,135))

plot(Vario)
plot(Variodir)

```

and a bubbleplot.

```{r full.lm_bubbleplot, fig.height=6, fig.width=8}
bubble.data<-data.frame(S.res.lm,data_sub$Longitude,data_sub$Latitude)
coordinates(bubble.data)<-c("data_sub.Longitude","data_sub.Latitude")

bubble(bubble.data,"S.res.lm",col=c("black","grey"),main="Residuals",xlab="Longitude",ylab="Latitude")

```

Both the variogram and the bubbleplot show that the residuals are spatially correlated. The directional variograms show that isotropy is a reasonable assumption so we can use a simple correlation structure in a gls framework to account for spatial dependency in the residuals.

###GLS choosing the correlation structure

Load the required library.

```{r nlme_lib}
library(nlme)
```

First we create a new dataframe with all the transformed variables.

```{r gls.data.1}
gls.data<-data.frame(Count_WW=log10(data_sub$Count_WW+1),Area_WHS=scale(data_sub$Area_WHS,scale=T),
                     Area_SPA=scale(data_sub$Area_SPA,scale=T), Dist_MSAC=scale(data_sub$Dist_MSAC,scale=T),
                     Area_SAC_L=scale(data_sub$Area_SAC_L,scale=T), Area_RAMSA=scale(data_sub$Area_RAMSA,scale=T),
                     Area_NR=scale(data_sub$Area_NR,scale=T), Area_NNR=scale(data_sub$Area_NNR,scale=T), 
                     Dist_MPA=scale(data_sub$Dist_MPA,scale=T), Dist_MCA=scale(data_sub$Dist_MCA,scale=T), 
                     Area_LNR=scale(data_sub$Area_LNR,scale=T), Area_COUNE=scale(data_sub$Area_COUNE,scale=T),
                     Area_CNTRY=scale(data_sub$Area_CNTRY,scale=T), Area_BIOSP=scale(data_sub$Area_BIOSP,scale=T), 
                     Area_BIOGE=scale(data_sub$Area_BIOGE,scale=T), Area_NP=scale(data_sub$Area_NP,scale=T), 
                     Dist_Air=scale(data_sub$Dist_Air,scale=T), Count_Bus=scale(data_sub$Count_Bus,scale=T),
                     Count_Hotel=log10(data_sub$Count_Hotel+1), Dist_CarPark=scale(data_sub$Dist_CarPark,scale=T), 
                     Dist_TourOp=scale(data_sub$Dist_TourOp,scale=T), Dist_Train=scale(data_sub$Dist_Train,scale=T), 
                     Dist_road=scale(data_sub$Dist_road,scale=T), Mean_Nat=scale(data_sub$Mean_Nat,scale=T),
                     Pop_dens=log10(data_sub$Pop_dens+1),x=data_sub$Longitude,y=data_sub$Latitude)

```

Then we fit the full model with gls without any correlation structure.

```{r gls.full.1, cache=TRUE}
full.gls<-gls(Count_WW~ Area_WHS +  Area_SPA + Dist_MSAC +Area_SAC_L + Area_RAMSA+ Area_NR +
                Area_NNR+ Dist_MPA + Dist_MCA + Area_LNR + Area_COUNE + Area_CNTRY + Area_BIOSP +
                Area_BIOGE + Area_NP + Dist_Air + Count_Bus + Count_Hotel + Dist_CarPark + Dist_TourOp +
                Dist_Train  + Dist_road + Mean_Nat + offset(Pop_dens),data=gls.data)

```

Now we fit the same model with different autocorrelation structures.

```{r gls.full.2, cache=TRUE}
full.gls.sph<-gls(Count_WW~ Area_WHS +  Area_SPA + Dist_MSAC +Area_SAC_L + Area_RAMSA+ Area_NR +
                    Area_NNR+ Dist_MPA + Dist_MCA + Area_LNR + Area_COUNE + Area_CNTRY + Area_BIOSP +
                    Area_BIOGE + Area_NP + Dist_Air + Count_Bus + Count_Hotel + Dist_CarPark + Dist_TourOp +
                    Dist_Train  + Dist_road + Mean_Nat + offset(Pop_dens),data=gls.data,
                  correlation=corSpher(form=~x+y,nugget=T))

full.gls.Lin<-gls(Count_WW~ Area_WHS +  Area_SPA + Dist_MSAC +Area_SAC_L + Area_RAMSA+ Area_NR +
                    Area_NNR+ Dist_MPA + Dist_MCA + Area_LNR + Area_COUNE + Area_CNTRY + Area_BIOSP +
                    Area_BIOGE + Area_NP + Dist_Air + Count_Bus + Count_Hotel + Dist_CarPark + Dist_TourOp +
                    Dist_Train  + Dist_road + Mean_Nat + offset(Pop_dens),data=gls.data,
                  correlation=corLin(form=~x+y,nugget=T))

full.gls.ratio<-gls(Count_WW~ Area_WHS +  Area_SPA + Dist_MSAC +Area_SAC_L + Area_RAMSA+ Area_NR +
                      Area_NNR+ Dist_MPA + Dist_MCA + Area_LNR + Area_COUNE + Area_CNTRY + Area_BIOSP +
                      Area_BIOGE + Area_NP + Dist_Air + Count_Bus + Count_Hotel + Dist_CarPark + Dist_TourOp +
                      Dist_Train  + Dist_road + Mean_Nat + offset(Pop_dens),data=gls.data,
                    correlation=corRatio(form=~x+y,nugget=T))

full.gls.Gaus<-gls(Count_WW~ Area_WHS +  Area_SPA + Dist_MSAC +Area_SAC_L + Area_RAMSA+ Area_NR +
                     Area_NNR+ Dist_MPA + Dist_MCA + Area_LNR + Area_COUNE + Area_CNTRY + Area_BIOSP +
                     Area_BIOGE + Area_NP + Dist_Air + Count_Bus + Count_Hotel + Dist_CarPark + Dist_TourOp +
                     Dist_Train  + Dist_road + Mean_Nat + offset(Pop_dens),data=gls.data,
                   correlation=corGaus(form=~x+y,nugget=T))

full.gls.exp<-gls(Count_WW~ Area_WHS +  Area_SPA + Dist_MSAC +Area_SAC_L + Area_RAMSA+ Area_NR +
                    Area_NNR+ Dist_MPA + Dist_MCA + Area_LNR + Area_COUNE + Area_CNTRY + Area_BIOSP +
                    Area_BIOGE + Area_NP + Dist_Air + Count_Bus + Count_Hotel + Dist_CarPark + Dist_TourOp +
                    Dist_Train  + Dist_road + Mean_Nat + offset(Pop_dens),data=gls.data,
                  correlation=corExp(form=~x+y,nugget=T))

```

Now we use AIC to choose the most appropriate correlation structure.

```{r corr_str}
AIC(full.gls,full.gls.sph,full.gls.Lin,full.gls.ratio,full.gls.Gaus,full.gls.exp)
```

The exponantial correlation structure seems to be the most appropriate for the data, therefore we refit the full model using this structure.
We use the method="ML" argument because we want to be able to compare the different models with AIC at the end.

First we create two new dataframes with the transformed variables and some aggregated variables that we will use during model selection.
The first dataframe contains the variable Area_SAC_L while the second contains Area_SSSI.

```{r gls.data.2}
gls.data.2<-data.frame(Count_WW=log10(data_sub$Count_WW+1),Area_WHS=scale(data_sub$Area_WHS,scale=T),
                       Dist_MSAC=scale(data_sub$Dist_MSAC,scale=T), Area_SPA=scale(data_sub$Area_SPA,scale=T),
                       Area_LSAC=scale(data_sub$Area_SAC_L,scale=T), Area_RAMSA=scale(data_sub$Area_RAMSA,scale=T),
                       Area_NR=scale(data_sub$Area_NR,scale=T), Area_NNR=scale(data_sub$Area_NNR,scale=T),
                       Dist_MPA=scale(data_sub$Dist_MPA,scale=T), Dist_MCA=scale(data_sub$Dist_MCA,scale=T),
                       Area_LNR=scale(data_sub$Area_LNR,scale=T), Area_COUNE=scale(data_sub$Area_COUNE,scale=T),
                       Area_CNTRY=scale(data_sub$Area_CNTRY,scale=T), Area_BIOSP=scale(data_sub$Area_BIOSP,scale=T),
                       Area_BIOGE=scale(data_sub$Area_BIOGE,scale=T), Area_NP=scale(data_sub$Area_NP,scale=T),
                       Dist_Air=scale(data_sub$Dist_Air,scale=T), Count_Bus=scale(data_sub$Count_Bus,scale=T),
                       Count_Hotel=log10(data_sub$Count_Hotel+1), Dist_CarPark=scale(data_sub$Dist_CarPark,scale=T),
                       Dist_TourOp=scale(data_sub$Dist_TourOp,scale=T), Dist_Train=scale(data_sub$Dist_Train,scale=T),
                       Dist_road=scale(data_sub$Dist_road,scale=T), Mean_Nat=scale(data_sub$Mean_Nat,scale=T),
                       Area_PA=scale(data_sub$Area_PA,scale=T),Count_Inf=log10(data_sub$Count_Inf+1),
                       Dist_MSAC=scale(data_sub$Dist_MSAC,scale=T),
                       Pop_dens=log10(data_sub$Pop_dens+1),x=data_sub$Longitude,y=data_sub$Latitude)

gls.data.3<-data.frame(Count_WW=log10(data_sub$Count_WW+1),Area_WHS=scale(data_sub$Area_WHS,scale=T),
                       Area_SSSI=scale(data_sub$Area_SSSI,scale=T), Area_SPA=scale(data_sub$Area_SPA,scale=T),
                       Dist_MSAC=scale(data_sub$Dist_MSAC,scale=T), Area_RAMSA=scale(data_sub$Area_RAMSA,scale=T),
                       Area_NR=scale(data_sub$Area_NR,scale=T), Area_NNR=scale(data_sub$Area_NNR,scale=T),
                       Dist_MPA=scale(data_sub$Dist_MPA,scale=T), Dist_MCA=scale(data_sub$Dist_MCA,scale=T),
                       Area_LNR=scale(data_sub$Area_LNR,scale=T), Area_COUNE=scale(data_sub$Area_COUNE,scale=T),
                       Area_CNTRY=scale(data_sub$Area_CNTRY,scale=T), Area_BIOSP=scale(data_sub$Area_BIOSP,scale=T),
                       Area_BIOGE=scale(data_sub$Area_BIOGE,scale=T), Area_NP=scale(data_sub$Area_NP,scale=T),
                       Dist_Air=scale(data_sub$Dist_Air,scale=T), Count_Bus=scale(data_sub$Count_Bus,scale=T),
                       Count_Hotel=log10(data_sub$Count_Hotel+1), Dist_CarPark=scale(data_sub$Dist_CarPark,scale=T),
                       Dist_TourOp=scale(data_sub$Dist_TourOp,scale=T), Dist_Train=scale(data_sub$Dist_Train,scale=T),
                       Dist_road=scale(data_sub$Dist_road,scale=T), Mean_Nat=scale(data_sub$Mean_Nat,scale=T),
                       Area_PA=scale(data_sub$Area_PA,scale=T),Count_Inf=log10(data_sub$Count_Inf+1),
                       Dist_MSAC=scale(data_sub$Dist_MSAC,scale=T),
                       Pop_dens=log10(data_sub$Pop_dens+1),x=data_sub$Longitude,y=data_sub$Latitude)



```

Now we fit the models with the two collinear variables and compared them with AIC.

```{r gls.full.3, cache=TRUE}
full.gls.exp.2<-gls(Count_WW~ Area_WHS + Area_LSAC +  Dist_MSAC +Dist_MPA +Dist_MCA +
                    Area_SPA +Area_RAMSA+ Area_NR + Area_NNR+   Area_LNR + Area_COUNE + 
                    Area_CNTRY + Area_BIOSP + Area_BIOGE + Area_NP + Dist_Air + Count_Bus + 
                    Count_Hotel + Dist_CarPark + Dist_TourOp + Dist_Train  + Dist_road + 
                    Mean_Nat + offset(Pop_dens),data=gls.data.2,
                    correlation=corExp(form=~x+y,nugget=T),method="ML")

full.gls.exp.3<-gls(Count_WW~ Area_WHS + Area_SSSI +  Dist_MSAC +Dist_MPA +Dist_MCA +
                    Area_SPA +Area_RAMSA+ Area_NR + Area_NNR+   Area_LNR + Area_COUNE + 
                    Area_CNTRY + Area_BIOSP + Area_BIOGE + Area_NP + Dist_Air + Count_Bus + 
                    Count_Hotel + Dist_CarPark + Dist_TourOp + Dist_Train  + Dist_road + 
                    Mean_Nat + offset(Pop_dens),data=gls.data.3,
                  correlation=corExp(form=~x+y,nugget=T),method="ML")

AIC(full.gls.exp.2,full.gls.exp.3)

```

The models are very similar but the model full.gls.exp.2 has an AIC slightly lower, for now we will use this model, but later in the model selection phase we will consider both.

We check model validity first.

```{r full.gls.valid.1}
res.gls<-residuals(full.gls.exp.2,type="normalized")
fit.gls<-fitted(full.gls.exp.2)

par(mfrow=c(1,2))
plot(res.gls~fit.gls)

qqnorm(res.gls)
qqline(res.gls)

```

We check that autocorrelation is not an issue anymore.

```{r full.gls.vario}
Vario.gls.exp<-Variogram(full.gls.exp.2,form= ~x+y,robust=T,resType = "normalized")

plot(Vario.gls.exp, smooth=F)

```

The residuals are no longer correlated and there are no visible patterns when compared to the fitted values.

Now we can look at the effects of the linear predictors.

```{r full.gls.exp.2.summary}
summary(full.gls.exp.2)
```

Some of the protected areas (WHS,marine SAC, SPA, NNR and LNR) are an attractor for the tourists, others (RAMSAR) have a negative effect on tourists numbers while others do not have an effect.
Among the different infrastructures, only the distance from the airport, and the number of hotels and bus stations seems to have an effect on the number of tourists, while the perceived naturalness of an area is a deterrent.

###Model selection

Now we can start selecting important variables for the model. 
First we can fit a model with aggregated variables: the total area of the cell that is occupied by a reserve, the total number of infrastructures and the mean naturalness.
This will test whether environmental, socio-economic infrastrcture or the naturalness is the strogest attractor for tourists.

```{r agg.gls, cache=TRUE}
agg.gls<-gls(Count_WW ~ Area_PA + Mean_Nat + Count_Inf + offset(Pop_dens),data=gls.data.2,
             correlation=corExp(form=~x+y,nugget=T),method="ML")

```

Check the validity of the model.

```{r agg.gls.valid}
res.agg.gls<-residuals(agg.gls,type="normalized")
fit.agg.gls<-fitted(agg.gls)

par(mfrow=c(1,2))
plot(res.agg.gls~fit.agg.gls)

qqnorm(res.agg.gls)
qqline(res.agg.gls)

```

Then look at the summary.

```{r agg.gls.summary}
summary(agg.gls)
```

All the variables seem to be imprtant in explaining the distribution of the wildlife watchers.
We can now fit an environmental model and an infrastructure model to select which variables are important in the two groups.

#####Environmental model

```{r env.gls, cache=TRUE}
env.gls<-gls(Count_WW~ Area_WHS + Area_LSAC +  Dist_MSAC +Dist_MPA +Dist_MCA +
               Area_SPA +Area_RAMSA+ Area_NR + Area_NNR+   Area_LNR + Area_COUNE + 
               Area_CNTRY + Area_BIOSP + Area_BIOGE + Area_NP + Mean_Nat + 
               offset(Pop_dens),data=gls.data.2, correlation=corExp(form=~x+y,nugget=T),method="ML")

```

Check model assumptions.

```{r env.gls.valid}
res.env.gls<-residuals(env.gls,type="normalized")
fit.env.gls<-fitted(env.gls)

par(mfrow=c(1,2))
plot(res.env.gls~fit.env.gls)

qqnorm(res.env.gls)
qqline(res.env.gls)

```

And summary.

```{r env.gls.summary}
summary(env.gls)
```

Results are similar to the full odel with the ecceptuion of RAMSAR not being significant any longer, CNTRY and NP now having a positive effect on number of tourists.

#####Infrastructure model

```{r inf.gls, cache=TRUE}
inf.gls<-gls(Count_WW ~ Dist_Air + Count_Bus + Count_Hotel + Dist_CarPark + Dist_TourOp +
               Dist_Train  + Dist_road +offset(Pop_dens),data=gls.data.2,
             correlation=corExp(form=~x+y,nugget=T),method="ML")

```

Model validation.

```{r inf.gls.valid}
res.inf.gls<-residuals(inf.gls,type="normalized")
fit.inf.gls<-fitted(inf.gls)

par(mfrow=c(1,2))
plot(res.inf.gls~fit.inf.gls)

qqnorm(res.inf.gls)
qqline(res.inf.gls)

```

And summary.

```{r inf.gls.summary}
summary(inf.gls)
```

The effects are the same as in the full model.

If we check with AIC which of these models explains the data best 

```{r gls.selection}
AIC(full.gls.exp.2, agg.gls, env.gls, inf.gls)
```

we can see that the full model is the best one, followed by the infrastructure model and the aggregated model.

###Model selection using dredge

We use the pdredge function in package MuMIn to select iportant variables in the models.

#####Infrastructures model

The function pdredge allows parallel computing.
The following code sets up a cluster and runs the dredge function on 3 nodes.

```{r pdredge, eval=FALSE}
library(parallel)
library(doParallel)
library(MuMIn)

cl <- makeCluster(3)            #split into 3 cores
registerDoParallel(cl)          #register the parallel backend
clusterExport(cl,"gls.data.2")  #export the dataframe to the cluster
clusterEvalQ(cl,library(nlme))  #load the required package onto the cluster

inf.sel<-pdredge(inf.gls,cluster=cl,rank = "AIC",trace=2)    #model selection for infrastructure model

saveRDS(inf.sel,"Infrastructure_sel.rds")

#pdredge(env.gls,cluster=cl,rank = "AIC",trace=2)    #model selection for environmental model

stopCluster(cl)

```

```{r readModSel, include=FALSE}
inf.sel<-readRDS("Infrastructure_sel.rds")
```

Take a subset of the models that are within 5 delta AIC from the best model and inspect the importance of each variable in this subset.

```{r VarImp}
library(MuMIn)
inf.sel.sub<-subset(inf.sel, delta <5)

inf.var.imp<-importance(inf.sel.sub)

```

Now we can use model averaging to get coefficient estimates averaged across the subset of models and calculate confidence intervals.

```{r ModAvg}
inf.avg<-model.avg(inf.sel, subset= delta < 5, revised.var = TRUE) 

inf.confint<-confint(inf.avg)

summary(inf.avg) 

```

Only the number of bus stations, hotels and the distance for an airport have a significant effect on the number of tourists that visit an area.

### Biodiversity 

The spatial distribution of the biodiversity records is overlapping that of Flickr data. There are areas that lack access infrastructure where people are not able to go to record biodiversity or for recreational purposes. So we exclude these areas from this analysis. Once we account for infrastructure and amenities, is species richness an attractor in areas that are easily accessible? 

We first look at the distribution of the biodiversity records.

```{r BioDistr}
par(mfrow=c(2,1))
hist(log(data_sub$Records+1),main="Histogram of species records",xlab="Species Records (log)")
plot(density(log(data_sub$Records+1)),main="Density of species records")
```

There is obviously a bimodal distribution with most of the cells having 0 records and then the distribution has another peak around 54.

Now we look at the distribution of species richness.

```{r SpecRichDist}
par(mfrow=c(2,1))
hist(log(data_sub$Species+1),main="Histogram of species richness",xlab="Species Richness (log)")
plot(density(log(data_sub$Species+1)),main="Density of species richness")
```

Very similar story for the species richness.


#### Subsetting
We can identify the two distributions that these data come from by using a mixture model.

```{r MixMod, eval=FALSE}
library(mclust)

#create variable log of number of records
data_sub$logrec<-log(data_sub$Records+1)

#Produces a density estimate for each data point using a Gaussian finite mixture model from Mclust.
clusrec<-densityMclust(data_sub$logrec,G=2)     #G=2 we know that there are 2 groups (low and high)

#plot the distribution
plot(clusrec, what = "density", data = data_sub$logrec, breaks = 15)

#Very high nuber of 0s 

#remove 0 records
sublogrec<-data_sub$logrec[data_sub$logrec>0]

#refit mixture model without 0s
clusrecsub<-densityMclust(sublogrec,G=2)

#plot
plot(clusrecsub, what = "density", data = sublogrec, breaks = 15)

#returns a summary of the model
classification<- summary(clusrecsub,classification=T)

#classification of each datapoint into the two groups is then in:
membership<-classification$classification

table(membership)
write.table(membership,"data/classification.txt")
```


Read classification from mixture model

```{r readClass}
classes<-read.table("data/classification.txt")
```


Now we use this classification to subset our dataset and only select records that are in an area that is accessible to people.

```{r Subset}
data_sub$logrec<-log(data_sub$Records+1)

#only select non 0 records
data.pos<-data_sub[data_sub$logrec>0,]

#now select only records belonging to group 2
data.gr2<-cbind(data.pos,classes)
data.gr2<-data.gr2[data.gr2$x==2,]
#str(data.gr2)
```

#### Effect of biodiversity

Now we can model the effect of biodiversity. First we try a simple linear model with both variables logtransformed.
We exclude the offset for population size because the exclusion of low records cells should already take care of that.

```{r BioLm}
bio.1<-lm(log10(Count_WW+1)~log10(Species),data=data.gr2)
summary(bio.1)
```

Check model assumptions.

```{r BioLmCheck}
par(mfrow=c(2,2))
plot(bio.1)
```

Check for spatial autocorrelation by extracting the standardised residuals ad plotting a variogram.


```{r BioLmSpatialCheck}
S.res.bio.1<-rstandard(bio.1)
#create a spatial dataframe
mydata_sp<-data.gr2
coordinates(mydata_sp)<-c("Longitude", "Latitude")

#calculate and plot variograms
Vario<-variogram(S.res.bio.1~1,data=mydata_sp)
Variodir<-variogram(S.res.bio.1~1,data=mydata_sp,alpha=c(0,45,90,135))

plot(Vario)
plot(Variodir)
```

The residuals are spatially correlated.

Use gls to include a crrelation structure. Since we are working on the same data we can use the same correlation structure that we used in the previous models.
We prepare the dataset and fit the model with both response and explanatory variables logtransformed.

```{r BioGlsData}
data.bio.gls<-data.frame(Count_WW=log10(data.gr2$Count_WW+1),Species=log10(data.gr2$Species),
                         x=data.gr2$Longitude,y=data.gr2$Latitude)
```

```{r BioGls, cache=TRUE}
bio.gls.exp<-gls(Count_WW~ Species,data=data.bio.gls, correlation=corExp(form=~x+y,nugget=T))
summary(bio.gls.exp)
```

We check for violations of model assumptions

```{r BioGlsCheck}
res.bio.gls<-residuals(bio.gls.exp, type="normalized")

fit.bio.gls<-fitted(bio.gls.exp)

par(mfrow=c(1,2))
plot(res.bio.gls ~ fit.bio.gls)

qqnorm(res.bio.gls)

qqline(res.bio.gls)
```


Check if autocorrelation was fixed.

```{r BioGlsSpatialCheck}
Vario<-variogram(res.bio.gls~1,data=mydata_sp)

plot(Vario)
```

No more autocorrelation.
We can check the fit by plotting predictions on top of the data.

```{r BioGlsPred, cache=TRUE}
# pred_data<-data.frame(Species=seq(from=min(data.bio.gls$Species), 
#                                   to=max(data.bio.gls$Species),by=0.001))
# 
# preds<-predict(bio.gls.exp,pred_data,type="link")
# preds<-as.data.frame(preds)
# 
# plot(Count_WW~Species,data=data.bio.gls)
# lines(preds$preds~pred_data$Species)
```

Not a good fit. The relationship does not look linear, therefore we can try fitting a gam to the logtransformed response with a spline for the number of species.

```{r BioGam}
library(mgcv)

bio.gam<-gam(log10( Count_WW +1 )~s(Species),data=data.gr2)
summary(bio.gam)
```

Diagnostic plots

```{r BioGamCheck}
gam.check(bio.gam)
```

Could be better.

Look at the fit.

```{r BioGamPred}
pred_data<-data.frame(Species=seq(from=min(data.gr2$Species), 
                                  to=max(data.gr2$Species),by=0.001))

preds<-predict(bio.gam,pred_data,type="response",se=T)
preds<-as.data.frame(preds)
CIup<-preds$fit + 1.96 *preds$se.fit
CIlow<-preds$fit - 1.96 *preds$se.fit


library(scales)

plot(log10(Count_WW +1)~Species,data=data.gr2, pch=20,col=alpha("cadetblue",0.5),xlab="Number of species",ylab="Number of Flickr users")
lines(preds$fit~pred_data$Species, lwd=3, col="cadetblue")
lines(CIup~pred_data$Species,lty=2,lwd=2,col="cadetblue")
lines(CIlow~pred_data$Species,lty=2,lwd=2,col="cadetblue")
```

Better fit.


